{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac72e308",
   "metadata": {},
   "source": [
    "# Week 2: Multi-Layer Perceptrons for Molecular Property Prediction\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Limitations of Linear Models\n",
    "`nn.Linear(5, 1)` can only learn straight-line relationships between inputs and outputs.\n",
    "Stacking multiple linear layers without activation functions reduces to a single linear transformation — depth adds nothing.\n",
    "Real molecular properties involve non-linear relationships between descriptors that a single linear layer cannot capture.\n",
    "\n",
    "### 2. Activation Functions\n",
    "Activation functions introduce non-linearity between linear layers, allowing the network to learn complex patterns.\n",
    "\n",
    "**ReLU** (Rectified Linear Unit):\n",
    "```\n",
    "ReLU(x) = max(0, x)\n",
    "```\n",
    "Negative inputs become zero, positive inputs pass through unchanged.\n",
    "Linear layers rotate and scale data — ReLU bends it. Repeated bending allows the network to approximate any relationship.\n",
    "\n",
    "### 3. Dropout\n",
    "Technique to prevent overfitting: randomly deactivates neurons during training, forcing the remaining neurons to learn the relationship independently.\n",
    "- **During training:** neurons are randomly turned off\n",
    "- **During inference:** all neurons are active\n",
    "\n",
    "PyTorch handles this automatically via `model.train()` and `model.eval()`.\n",
    "\n",
    "### 4. Batch Normalisation\n",
    "Normalises layer outputs to mean ~0 and standard deviation ~1 across the batch before passing to the next layer.\n",
    "Keeps activations in a stable range throughout the network, preventing gradients from growing too large or too small.\n",
    "\n",
    "PyTorch offers three variants depending on the dimensionality of the input data:\n",
    "\n",
    "| Variant | Input Shape | Use Case |\n",
    "|---|---|---|\n",
    "| `nn.BatchNorm1d` | `(batch_size, features)` | Flat feature vectors — molecular descriptors, fingerprints |\n",
    "| `nn.BatchNorm2d` | `(batch_size, channels, height, width)` | Image feature maps — CNNs |\n",
    "| `nn.BatchNorm3d` | `(batch_size, channels, depth, height, width)` | Volumetric data — 3D protein structures |\n",
    "\n",
    "For MLPs working with molecular descriptors, always use `nn.BatchNorm1d`.\n",
    "`nn.BatchNorm2d` will appear later when working with CNNs for image-based molecular representations.\n",
    "\n",
    "\n",
    "Typical layer block in a modern MLP:\n",
    "```\n",
    "Linear → BatchNorm → ReLU → Dropout\n",
    "```\n",
    "\n",
    "### 5. Input Normalisation\n",
    "Molecular descriptors naturally span very different scales (MW: 200-500, HBD: 0-5).\n",
    "Without normalisation, large-valued descriptors dominate the gradients, causing unstable training.\n",
    "Normalising to mean 0 and std 1 puts all descriptors on equal footing.\n",
    "\n",
    "**Critical rule:** always normalise validation and test data using training set statistics:\n",
    "```python\n",
    "mean = X_train.mean(dim=0)\n",
    "std = X_train.std(dim=0)\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_val = (X_val - mean) / std   # training stats, not val stats\n",
    "X_test = (X_test - mean) / std # training stats, not test stats\n",
    "```\n",
    "At deployment, only training statistics are available — this is the only feasible approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eca3866",
   "metadata": {},
   "source": [
    "## Part 1: MLP the Hard Way — Custom nn.Module\n",
    "\n",
    "Before using `nn.Sequential`, we build a 3-layer MLP manually using a custom `nn.Module` class.\n",
    "This makes the forward pass explicit, so we can see exactly how data flows through the network\n",
    "before abstracting it away.\n",
    "\n",
    "Architecture:\n",
    "- Input layer: 5 → 16 neurons + ReLU\n",
    "- Hidden layer: 16 → 16 neurons + ReLU\n",
    "- Output layer: 16 → 1 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() # Call the parent class's __init__ to initialize the module for its intrinsic function call\n",
    "        # layer 1: input 5 features\n",
    "        self.layer1 = nn.Linear(5, 16) # input layer with 5 features and 16 hidden neurons\n",
    "        # layer 2: hidden layer with 16 neurons + ReLU\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(16, 16) # hidden layer with 16 neurons\n",
    "        # layer 3: output layer with 1 neuron \n",
    "        self.layer3 = nn.Linear(16, 1) # output layer with 1 neuron\n",
    "    \n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x: input tensor of shape (batch_size, 5)\n",
    "        Total number of parameters in the model:\n",
    "        Layer 1: (5 weights + 1 bias) * 16 neurons = 96 parameters\n",
    "        Layer 2: (16 weights + 1 bias) * 16 neurons = 272 parameters\n",
    "        Layer 3: (16 weights + 1 bias) * 1 neuron = 17 parameters\n",
    "        Total parameters = 96 + 272 + 17 = 385 parameters\n",
    "        '''\n",
    "        x = self.layer1(x) # 16 independent linear transformations were made: 16 groups of (5 weights + bias) each were used\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x) # 16 independent linear transformations were made: 16 groups of (16 weights + bias) each were used\n",
    "        x = self.relu(x)\n",
    "        x = self.layer3(x) # 1 independent linear transformation was made: 1 group of (16 weights + bias) each were used\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "Total number of parameters:  385\n"
     ]
    }
   ],
   "source": [
    "# Dummy input to test the model\n",
    "model = MLP()\n",
    "x = torch.rand(10, 5) \n",
    "out = model(x)\n",
    "print(out.shape) \n",
    "print(\"Total number of parameters: \", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e0aca",
   "metadata": {},
   "source": [
    "## Part 2: MLP the Easy Way — nn.Sequential\n",
    "\n",
    "Refactoring the same architecture using `nn.Sequential`.\n",
    "The forward pass becomes implicit — data flows through layers in the order they are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b640fd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "Total number of parameters:  385\n"
     ]
    }
   ],
   "source": [
    "simplified_model = nn.Sequential(\n",
    "    nn.Linear(5, 16), # input layer with 5 features and 16 hidden neurons\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 16), # hidden layer with 16 neurons\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 1) # output layer with 1 neuron\n",
    ")\n",
    "\n",
    "x = torch.rand(10, 5) \n",
    "out = simplified_model(x)\n",
    "print(out.shape) \n",
    "print(\"Total number of parameters: \", sum(p.numel() for p in simplified_model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081807f2",
   "metadata": {},
   "source": [
    "## Part 3: Full MLP for Molecular Property Prediction\n",
    "\n",
    "Scaling up to the B1 project architecture with:\n",
    "- Input: 200 features (RDKit descriptors)\n",
    "- Hidden layer 1: 512 neurons + BatchNorm + ReLU + Dropout(0.2)\n",
    "- Hidden layer 2: 256 neurons + BatchNorm + ReLU + Dropout(0.2)\n",
    "- Hidden layer 3: 128 neurons + BatchNorm + ReLU + Dropout(0.2)\n",
    "- Output: 1 neuron\n",
    "\n",
    "\n",
    "Order for setting up a layer: Linear → BatchNorm → ReLU → Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc08bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building model\n",
    "\n",
    "full_model = nn.Sequential(\n",
    "    # --- Block 1 ---\n",
    "    # Projects 200 raw input features (data, not neurons) into 512 neurons\n",
    "    nn.Linear(200, 512),\n",
    "    nn.BatchNorm1d(512),  # normalises the 512 neuron outputs\n",
    "    nn.ReLU(),            # introduces non-linearity\n",
    "    nn.Dropout(0.2),      # randomly deactivates 20% of neurons during training\n",
    "\n",
    "    # --- Block 2 ---\n",
    "    # Hidden layer: 512 neurons → 256 neurons\n",
    "    nn.Linear(512, 256),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    # --- Block 3 ---\n",
    "    # Hidden layer: 256 neurons → 128 neurons\n",
    "    nn.Linear(256, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "\n",
    "    # --- Output ---\n",
    "    # No activation, BatchNorm, or Dropout — raw value for regression\n",
    "    nn.Linear(128, 1)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acff7889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n",
      "Total parameters: 269057\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(16, 200)\n",
    "out = full_model(x)\n",
    "print(out.shape)  # should be torch.Size([16, 1])\n",
    "print(\"Total parameters:\", sum(p.numel() for p in full_model.parameters())) # 269057 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e94ad5",
   "metadata": {},
   "source": [
    "## Parameter Count\n",
    "\n",
    "Total learnable parameters: 269,057\n",
    "\n",
    "### Breakdown\n",
    "\n",
    "| Layer | Calculation | Parameters |\n",
    "|---|---|---|\n",
    "| `Linear(200, 512)` | 512 × 200 + 512 (bias) | 102,912 |\n",
    "| `BatchNorm1d(512)` | 512 × 2 (gamma + beta) | 1,024 |\n",
    "| `Linear(512, 256)` | 256 × 512 + 256 (bias) | 131,328 |\n",
    "| `BatchNorm1d(256)` | 256 × 2 (gamma + beta) | 512 |\n",
    "| `Linear(256, 128)` | 128 × 256 + 128 (bias) | 32,896 |\n",
    "| `BatchNorm1d(128)` | 128 × 2 (gamma + beta) | 256 |\n",
    "| `Linear(128, 1)` | 1 × 128 + 1 (bias) | 129 |\n",
    "| **Total** | | **269,057** |\n",
    "\n",
    "### Notes\n",
    "- `nn.Linear(in, out)` parameters: `out × in` (weights) + `out` (biases)\n",
    "- `nn.BatchNorm1d(n)` parameters: `n × 2` — one **gamma** (scale) and one **beta** (shift) per feature\n",
    "- BatchNorm normalises to mean 0 and std 1, then learns the optimal scale and shift via gamma and beta\n",
    "- `nn.ReLU()` and `nn.Dropout()` have no learnable parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
