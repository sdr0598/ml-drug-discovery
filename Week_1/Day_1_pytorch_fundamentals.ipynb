{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeTT5UzEoQCdm2DAqaUx55",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdr0598/ml-drug-discovery/blob/main/week1_pytorch_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 1 - Day 1: PyTorch Fundamentals\n",
        "## Key Concepts\n",
        "- Tensor: multi-dimensional array that can live on GPU\n",
        "- requires_grad: tells PyTorch to track operations for differentiation\n",
        "- loss.backward(): computes gradients automatically\n",
        "- w.grad.zero_(): must zero gradients each step or they accumulate\n",
        "\n",
        "## Drug Discovery Connection\n",
        "- Rows of a 2D tensor = molecules\n",
        "- Columns = features (descriptors, fingerprints)\n",
        "- The training loop is how all models (SVM was manual, NN is automatic) learn"
      ],
      "metadata": {
        "id": "te5-d5g7UFJT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i88zB6U-mAM",
        "outputId": "55ef1bbf-bf3b-48ad-e9ab-daad0be8f673"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cpu\n",
            "GPU available: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"GPU available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensors are arrays but smarter\n",
        "# Same concept as NumPy arrays but can run on GPU and track gradients\n",
        "import torch\n",
        "\n",
        "# Create tensors\n",
        "x = torch.tensor([1.0, 2.0, 3.0])\n",
        "print(x)\n",
        "print(x.shape)\n",
        "print(x.dtype)\n",
        "\n",
        "# 2D tensor (think: a matrix of molecular descriptors)\n",
        "# Rows = molecules, Columns = features\n",
        "descriptors = torch.zeros(10, 100)\n",
        "print(descriptors.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtHyZxxl_h88",
        "outputId": "0384e79d-3613-4bdb-b158-13fb3fc6b22d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3.])\n",
            "torch.Size([3])\n",
            "torch.float32\n",
            "torch.Size([10, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd: Makes PyTorch special\n",
        "# requires_grad=True tells PyTorch: \"track everything done to this tensor\"\n",
        "# This is how neurals networks learn - automatic differentiation\n",
        "\n",
        "w = torch.tensor(2.0, requires_grad=True) # model weight??\n",
        "x = torch.tensor(3.0)\n",
        "\n",
        "y = w * x + 1 # y = 2 * 3 + 1 = 7\n",
        "print(\"y =\", y)\n",
        "\n",
        "y.backward()\n",
        "print(\"dy/dw\", w.grad) #\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4QEi1-nAoZY",
        "outputId": "4ccf388b-aacd-4d51-e425-09f7339414fd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = tensor(7., grad_fn=<AddBackward0>)\n",
            "dy/dw tensor(3.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prototype of \"Binding Affinity Predictor\"\n",
        "# Input: One molecule descriptor value\n",
        "# Output: predicted affinity\n",
        "# Goal: Find the best weight w that minimizes prediction error\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Dummy \"true\" relationship: affinity = 0.5 * descriptor + noise\n",
        "true_w = 0.5\n",
        "X = torch.randn(20) # Tensor: 1 row, 20 columns; rows = features; columns = molecules\n",
        "y_true = true_w*X + torch.randn(20)*0.1\n",
        "\n",
        "# Start model with a random weight\n",
        "w = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "#Training loop = core of ALL neural network training\n",
        "learning_rate = 0.1\n",
        "for epoch in range(20):\n",
        "  # Forward pass: make prediction\n",
        "  y_pred = w *X\n",
        "\n",
        "  # Compute loss (how far off are we?)\n",
        "  loss = ((y_pred - y_true) **2).mean()\n",
        "\n",
        "  # Backward pass: compute gradients\n",
        "  loss.backward()\n",
        "\n",
        "  # Update weight (gradient descent)\n",
        "  with torch.no_grad():\n",
        "  # Changes made to w does not count as a forward pass\n",
        "  # no_grad() ensures this\n",
        "  # Otherwise, this would be recorded into the computation graph and PyTorch would try to differentiate it\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  # Zero gradients (Important - must do this every step)\n",
        "  w.grad.zero_()\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "    print(f\"Epoch {epoch}: loss={loss.item():.4f}, w={w.item():.4f}\")\n",
        "\n",
        "print(f\"\\nLearned w={w.item():.4f}, true w={true_w}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djCvH27oLTTw",
        "outputId": "f9af7aeb-ed0c-44d0-98d9-86886772dfe8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.2649, grad_fn=<MeanBackward0>)\n",
            "tensor(0.1001, requires_grad=True)\n",
            "Epoch 0: loss=0.2649, w=0.1001\n",
            "tensor(0.0366, grad_fn=<MeanBackward0>)\n",
            "tensor(0.3744, requires_grad=True)\n",
            "Epoch 5: loss=0.0366, w=0.3744\n",
            "tensor(0.0102, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4677, requires_grad=True)\n",
            "Epoch 10: loss=0.0102, w=0.4677\n",
            "tensor(0.0072, grad_fn=<MeanBackward0>)\n",
            "tensor(0.4993, requires_grad=True)\n",
            "Epoch 15: loss=0.0072, w=0.4993\n",
            "\n",
            "Learned w=0.5088, true w=0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZ3ce_DgCzuT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
