{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b947b125",
   "metadata": {},
   "source": [
    "#Goal: Understand the concept behind .backward() method\n",
    "Synopsis:\n",
    "Gradient will be calculated in 2 ways and confirm if they match:\n",
    "(i) Numerical gradient (Calculus- approximation of differentation operator)\n",
    "(ii) Analytical gradient (via Autograd- method used by PyTorch)\n",
    "Note: Catastrophic cancellation - Digits beyond 7 SF in 32-bit floats are prone to noise and are unreliable. The numerical gradient method is susceptible to this whereas analytical gradient (Autograd) is not because subtracting two nearly identical loss values cancels meaningful digits, and dividing by a tiny epsilon amplifies the remaining noise.\n",
    "This is a core motivation for autograd â€” it computes exact analytical gradients regardless of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38741af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient: 1.720351\n",
      "Numerical gradient:  1.720786\n",
      "Difference:          4.35e-04\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(50) # one descriptor across 50 molecules\n",
    "y_true = 0.7 * x + 0.1\n",
    "\n",
    "w = torch.tensor(1.5, requires_grad=True) # weight\n",
    "\n",
    "# Analytical gradient via autograd\n",
    "y_pred = w * x\n",
    "loss = ((y_pred - y_true) ** 2).mean()\n",
    "loss.backward()\n",
    "analytical_gradient = w.grad.item()\n",
    "\n",
    "# Numerical gradient \n",
    "epsilon = 1e-4\n",
    "with torch.no_grad():\n",
    "    loss_plus = (((w + epsilon) * x - y_true) ** 2).mean()\n",
    "    loss_minus = (((w - epsilon) * x - y_true) ** 2).mean()\n",
    "numerical_gradient = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "\n",
    "print(f\"Analytical gradient: {analytical_gradient:.6f}\")\n",
    "print(f\"Numerical gradient:  {numerical_gradient:.6f}\")\n",
    "print(f\"Difference:          {abs(analytical_gradient - numerical_gradient):.2e}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65cc933d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eps=1e-01  diff=9.54e-07\n",
      "0.3440704345703125\n",
      "eps=1e-02  diff=3.22e-06\n",
      "0.034406960010528564\n",
      "eps=1e-03  diff=1.07e-04\n",
      "0.0034409165382385254\n",
      "eps=1e-04  diff=4.35e-04\n",
      "0.00034415721893310547\n",
      "eps=1e-05  diff=2.22e-03\n",
      "3.445148468017578e-05\n",
      "eps=1e-07  diff=3.66e-01\n",
      "4.172325134277344e-07\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for eps in [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-7]:\n",
    "        loss_plus  = (((w + eps) * x - y_true) ** 2).mean()\n",
    "        loss_minus = (((w - eps) * x - y_true) ** 2).mean()\n",
    "        num_grad = ((loss_plus - loss_minus) / (2 * eps)).item()\n",
    "        print(f\"eps={eps:.0e}  diff={abs(analytical_gradient - num_grad):.2e}\")\n",
    "        print((loss_plus - loss_minus).item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
