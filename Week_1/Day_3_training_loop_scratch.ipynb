{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2b6ab47",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "Implement a complete training loop using only tensors -- no PyTorch abstractions.\n",
    "This exercise will help understand every step involved before hiding them behind nn.Module in upcoming sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ebe44c",
   "metadata": {},
   "source": [
    "# 3 Takeaways\n",
    "(i) Training loop: Ground truth, model + starting values for weight & bias, forward pass + loss +  backward propagation + parameter update (wrapped around torch.no_grad)\n",
    "(ii) nn.Module offers abstraction for all the steps involved in the training loop, simplifying model setup + training (optimizer.step automatically invokes torch.no_grad to update the weights)\n",
    "(iii) Adam & Stochastic Gradient descent (SGD) are options available for tuning parameters. SGD best for simple linear model while adam for complex models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5088b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.3047, w=0.1109, b=0.0330\n",
      "Epoch 10: loss=0.0103, w=0.4670, b=0.1172\n",
      "Epoch 20: loss=0.0085, w=0.4966, b=0.1181\n",
      "Epoch 30: loss=0.0084, w=0.4992, b=0.1175\n",
      "Epoch 40: loss=0.0084, w=0.4995, b=0.1174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Data: 50 molecules, 1 dummy descriptor, predicting binding affinity\n",
    "x = torch.randn(50)\n",
    "y_true = 0.5 * x + 0.1 + torch.randn(50) * 0.1\n",
    "\n",
    "# Initialize weight and bias manually\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(50):\n",
    "    # Forward pass\n",
    "    y_pred = w * x + b\n",
    "\n",
    "    # loss\n",
    "    loss = ((y_pred - y_true) ** 2).mean()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward() \n",
    "    # Note in this case, 2 independent partial derivatives are calculated: dloss/dw and dloss/db \n",
    "    # (aka with respect to all tensors with requires_grad=True)\n",
    "\n",
    "    # Weight update\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        b -= learning_rate * b.grad\n",
    "    \n",
    "    # Zero gradients\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: loss={loss.item():.4f}, w={w.item():.4f}, b={b.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33e055d",
   "metadata": {},
   "source": [
    "# Couple of things worth noting from the output\n",
    "-loss plateaus after epoch 20. It's not reaching 0 due to noise term added to the y_true dataset.\n",
    "-A perfect model cannot recover noise\n",
    "-So, in the context of drug discovery, a model will have an irreducible error floor set by the quality of the experimental data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b47819",
   "metadata": {},
   "source": [
    "## Refactor: Raw Tensors -> nn.Module + torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbfcb596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss=0.5785, w=0.7141, b=0.6886\n",
      "Epoch 10: loss=0.0142, w=0.5239, b=0.1798\n",
      "Epoch 20: loss=0.0072, w=0.5005, b=0.1239\n",
      "Epoch 30: loss=0.0071, w=0.4977, b=0.1177\n",
      "Epoch 40: loss=0.0071, w=0.4973, b=0.1170\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Set up linear regression model\n",
    "model = nn.Linear(1,1)\n",
    "# Data: 50 molecules, 1 dummy descriptor, predicting binding affinity\n",
    "x = torch.randn(50, 1)  # Reshape to (50, 1) for nn.Linear\n",
    "y_true = 0.5 * x + 0.1 + torch.randn(50, 1) * 0.1\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "# Training loop\n",
    "for epoch in range(50):\n",
    "    y_pred = model(x)  # Forward pass\n",
    "    loss = criterion(y_pred, y_true)  # Compute loss\n",
    "    optimizer.zero_grad()  # Zero gradients\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update weights\n",
    "    if epoch % 10 == 0:\n",
    "        '''\n",
    "        Avoid unpacking the parameters directly. Not robust\n",
    "        w, b = model.parameters()\n",
    "        print(f\"Epoch {epoch}: loss={loss.item():.4f}, w={w.item():.4f}, b={b.item():.4f}\")\n",
    "        '''\n",
    "        # Instead, access the parameters by name for better readability and robustness\n",
    "        print(f\"Epoch {epoch}: loss={loss.item():.4f}, w={model.weight.item():.4f}, b={model.bias.item():.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
