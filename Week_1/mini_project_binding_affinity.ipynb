{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55be99a6",
   "metadata": {},
   "source": [
    "# Mini Project: Linear Binding Affinity Predictor\n",
    "\n",
    "## Goal\n",
    "Build a linear binding affinity predictor using multiple descriptors — extending everything from this week into one cohesive pipeline.\n",
    "\n",
    "## Specifications\n",
    "- 100 molecules, 5 descriptors (MW, logP, TPSA, HBD, HBA — dummy values for now)\n",
    "- True relationship: `affinity = 0.3*MW - 0.5*logP + 0.1*TPSA + 0.2*HBD - 0.4*HBA + noise`\n",
    "- `MoleculeDataset` + `DataLoader` with batch size 16\n",
    "- `nn.Linear(5, 1)` model\n",
    "- SGD optimizer, MSELoss\n",
    "- 100 epochs, print epoch loss every 10\n",
    "- At the end, print learned weights vs true weights\n",
    "\n",
    "**Challenge:** with 5 input features, `nn.Linear(5, 1)` has 5 weights — one per descriptor.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Reminders\n",
    "\n",
    "### 1. Check tensor shapes before computing loss\n",
    "`model(descriptors)` outputs shape `(batch_size, 1)` while labels are `(batch_size,)`.\n",
    "Use `.squeeze(1)` to align shapes before passing to the loss function.\n",
    "\n",
    "### 2. Divide epoch loss by number of batches, not batch size\n",
    "`epoch_loss` accumulates the sum of per-batch losses across the epoch:\n",
    "```\n",
    "epoch_loss = loss_batch_1 + loss_batch_2 + ... + loss_batch_N\n",
    "```\n",
    "Each batch loss is already averaged over molecules within that batch by `nn.MSELoss()`.\n",
    "Dividing by `len(dataloader)` gives the mean batch loss across the epoch — a stable, comparable number.\n",
    "Dividing by batch size has no clean interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468d3201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class for Batching\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class BindingAffinityDataset(Dataset):\n",
    "    def __init__(self, descriptors, affinities):\n",
    "        self.descriptors = descriptors\n",
    "        self.affinities = affinities\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.descriptors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.descriptors[idx], self.affinities[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3736b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dummy descriptors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "dummy_descriptors = torch.randn(100,5) # 100 molecules, 5 descriptors each: MW, logP, TPSA, HBD, HBA\n",
    "\n",
    "# True relationship: affinity = 0.3*MW - 0.5*logP + 0.1*TPSA + 0.2*HBD - 0.4*HBA + noise\n",
    "mw, logp, tpsa, hbd, hba = 0.3, -0.5, 0.1, 0.2, -0.4\n",
    "true_weights = torch.tensor([mw, logp, tpsa, hbd, hba])\n",
    "noise = 0.1 * torch.randn(100)\n",
    "\n",
    "affinities = dummy_descriptors @ true_weights + noise\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = BindingAffinityDataset(dummy_descriptors, affinities)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b3404b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.5504\n",
      "Learned weights: MW=0.2631, logP=0.0708, TPSA=-0.2832, HBD=0.0474, HBA=-0.4226\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n",
      "Epoch 10, Loss: 0.0582\n",
      "Learned weights: MW=0.2412, logP=-0.3571, TPSA=-0.0525, HBD=0.1548, HBA=-0.3961\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n",
      "Epoch 20, Loss: 0.0131\n",
      "Learned weights: MW=0.2740, logP=-0.4651, TPSA=0.0405, HBD=0.1868, HBA=-0.3928\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n",
      "Epoch 30, Loss: 0.0093\n",
      "Learned weights: MW=0.2901, logP=-0.4983, TPSA=0.0731, HBD=0.1948, HBA=-0.3948\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n",
      "Epoch 40, Loss: 0.0082\n",
      "Learned weights: MW=0.2954, logP=-0.5101, TPSA=0.0874, HBD=0.1989, HBA=-0.3936\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n",
      "Epoch 50, Loss: 0.0086\n",
      "Learned weights: MW=0.2976, logP=-0.5131, TPSA=0.0880, HBD=0.1962, HBA=-0.3921\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n",
      "Epoch 60, Loss: 0.0080\n",
      "Learned weights: MW=0.2986, logP=-0.5142, TPSA=0.0916, HBD=0.1978, HBA=-0.3945\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n",
      "Epoch 70, Loss: 0.0078\n",
      "Learned weights: MW=0.3007, logP=-0.5119, TPSA=0.0938, HBD=0.1986, HBA=-0.3957\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n",
      "Epoch 80, Loss: 0.0079\n",
      "Learned weights: MW=0.2995, logP=-0.5093, TPSA=0.0930, HBD=0.1976, HBA=-0.3944\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n",
      "Epoch 90, Loss: 0.0075\n",
      "Learned weights: MW=0.3006, logP=-0.5145, TPSA=0.0946, HBD=0.1968, HBA=-0.3955\n",
      "True weights: MW=0.3000, logP=-0.5000, TPSA=0.1000, HBD=0.2000, HBA=-0.4000\n"
     ]
    }
   ],
   "source": [
    "# Set up a linear regression model via neural network\n",
    "model = nn.Linear(5, 1) # 5 input features, 1 output (binding affinity)\n",
    "\n",
    "# Define loss function and Single Gradient Descent optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for batch in dataloader:\n",
    "        descriptors, affinities_true = batch\n",
    "        affinities_pred = model(descriptors)\n",
    "        # affinities_true: [16], affinities_pred: [16, 1]\n",
    "        affinities_pred = affinities_pred.squeeze(1) # Reshape to [16]\n",
    "        # print(affinities_true.shape, affinities_pred.shape) # Check shapes\n",
    "        loss = criterion(affinities_pred, affinities_true)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if epoch % 10 == 0:\n",
    "        # epoch_loss = loss_batch_1 + loss_batch_2 + ... + loss_batch_16\n",
    "        print(f'Epoch {epoch}, Loss: {epoch_loss/len(dataloader):.4f}')\n",
    "        # Unpack learned weights and compare to true weights\n",
    "        mw_, logP_, tpsa_, hbd_, hba_ = model.weight.data[0]\n",
    "        print(f'Learned weights: MW={mw_:.4f}, logP={logP_:.4f}, TPSA={tpsa_:.4f}, HBD={hbd_:.4f}, HBA={hba_:.4f}')\n",
    "        print(f'True weights: MW={mw:.4f}, logP={logp:.4f}, TPSA={tpsa:.4f}, HBD={hbd:.4f}, HBA={hba:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
